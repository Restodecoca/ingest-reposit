# The Llama Cloud API key.
# LLAMA_CLOUD_API_KEY=

# The provider for the AI models to use.
MODEL_PROVIDER=openai

# The name of LLM model to use.
MODEL=gpt-4o-mini

# Name of the embedding model to use.
EMBEDDING_MODEL=text-embedding-3-small #note: small is 6.5x cheaper than larger

# Dimension of the embedding model to use.
EMBEDDING_DIM=1024

# The questions to help users get started (multi-line).
# CONVERSATION_STARTERS=

# The OpenAI API key to use.
OPENAI_API_KEY= # Your OpenAI API key

# Temperature for sampling from the model.
# LLM_TEMPERATURE=

# Maximum number of tokens to generate.
# LLM_MAX_TOKENS=

# The number of similar embeddings to return when retrieving documents.
TOP_K=2

# The name of the collection in your Chroma database
CHROMA_COLLECTION=dense_vectors

# The hostname for your Chroma database. Eg: localhost
# CHROMA_HOST=

# The port for your Chroma database. Eg: 8000
# CHROMA_PORT=

# The local path to the Chroma database. 
# Specify this if you are using a local Chroma database. 
# Otherwise, use CHROMA_HOST and CHROMA_PORT config above
CHROMA_PATH=storage/chromadb

#Path for bm25
BM25_PATH=storage/bm25

# FILESERVER_URL_PREFIX is the URL prefix of the server storing the images generated by the interpreter.
FILESERVER_URL_PREFIX=http://localhost:8000/api/files

# The address to start the backend app.
APP_HOST=0.0.0.0

# The port to start the backend app.
APP_PORT=8000

# Customize prompt to generate the next question suggestions based on the conversation history.
# Disable this prompt to disable the next question suggestions feature.
NEXT_QUESTION_PROMPT="Você é um assistente prestativo! Sua tarefa é sugerir a próxima pergunta que o usuário pode fazer.  
Aqui está o histórico da conversa  
---------------------  
{conversation}  
---------------------  
Dado o histórico da conversa, forneça 3 perguntas que o usuário pode fazer a seguir!  

Sua resposta deve ser envolvida por três crases, seguindo o seguinte formato:  
```
<question 1>  
<question 2>  
<question 3>  
```"

# The system prompt for the AI model.
SYSTEM_PROMPT="You are a helpful assistant who helps users with their questions.
#You have access to a knowledge base including the facts that you should start with to find the answer for the user question. Use the query engine tool to retrieve the facts from the knowledge base."

SYSTEM_CONTEXT_PROMPT="A seguir, apresenta-se uma conversa amistosa entre um usuário e um assistente de IA.
O assistente é comunicativo e fornece inúmeros detalhes específicos extraídos do seu contexto.
Aqui estão os arquivos relevantes para o contexto, relacionados ao Sicoob:
{context_str}
Instrução: Com base nos arquivos acima, forneça uma resposta detalhada para a pergunta do usuário a seguir, 
utilizando somente as informações presentes nos arquivos e sem recorrer a conhecimentos prévios ou gerais,
mesmo que de domínio comum. Responda (não sei) se a informação solicitada não constar nos arquivos ou se o tema da pergunta estiver fora do contexto."

# An additional system prompt to add citation when responding to user questions.
SYSTEM_CITATION_PROMPT='You have provided information from a knowledge base that has been passed to you in nodes of information.
Each node has useful metadata such as node ID, file name, page, etc.
Please add the citation to the data node for each sentence or paragraph that you reference in the provided information.
The citation format is: . [citation:<node_id>]()
Where the <node_id> is the unique identifier of the data node.

Example:
We have two nodes:
  node_id: xyz
  file_name: llama.pdf
  
  node_id: abc
  file_name: animal.pdf

User question: Tell me a fun fact about Llama.
Your answer:
A baby llama is called "Cria" [citation:xyz]().
It often live in desert [citation:abc]().
It\'s cute animal.
'

MYSQL_HOST=localhost
MYSQL_USER=root
MYSQL_PORT=3306
MYSQL_DATABASE=
MYSQL_TABLE=chatstore
MYSQL_PASSWORD=


CREDENTIALS=#google credentials
TOKEN= #google token
DRIVE_FOLDER= #drive folder id